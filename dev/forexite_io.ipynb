{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# csv_to_postgres.py\n",
    "Script for loading comma seperated text files - e.g. downloaded from website forexite.com with the script *get_forexit_data.ipynb* - into the forex database.\n",
    "\n",
    "Text files are supposed to be named by date, e.g. 280416.txt contains forex data for 28th of April, 2016.\n",
    "\n",
    "### Parameters:\n",
    "- dir_from : location of txt files \n",
    "- start_date: date for first file to be uploaded, e.g. date(2015, 7, 7)\n",
    "- end_date: date **plus 1 day** for last file to be uploaded, e.g. date(2016, 4, 28) uploads up to April 27th, 2016.\n",
    "- current directory (where you run the program): **needs to be update manually in the code on line with engine.execute (still hard coded).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "from datetime import date, datetime, timedelta\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql://pieter:root@localhost:5432/forex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set parameters\n",
    "dir_from = '/home/pieter/projects/quantfxengine/temp/'\n",
    "start_date = date(2016, 5, 1)\n",
    "end_date = date(2016, 6, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you only need to upload this once to Postgresql!\n",
    "\"\"\"\n",
    "df_currencygroup = pd.read_excel(\"/home/pieter/forex/Currency_pair_overview.xls\", sheetname=\"CurrencyGroup\")\n",
    "df_currency = pd.read_excel(\"/home/pieter/forex/Currency_pair_overview.xls\", sheetname=\"Currency\")\n",
    "df_currencypair = pd.read_excel(\"/home/pieter/forex/Currency_pair_overview.xls\", sheetname=\"CurrencyPair\")\n",
    "\n",
    "df_currencygroup.to_sql('tbl_currency_group', engine, schema=None, if_exists='append', index=False, index_label=None, chunksize=None, dtype=None)\n",
    "df_currency.to_sql('tbl_currency', engine, schema=None, if_exists='append', index=False, index_label=None, chunksize=None, dtype=None)\n",
    "df_currencypair.to_sql('tbl_currency_pair', engine, schema=None, if_exists='append', index=False, index_label=None, chunksize=None, dtype=None)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def datespan(startDate, endDate, delta=timedelta(days=1)):\n",
    "    \"\"\" iterator to iterate through files per day.\n",
    "    \"\"\"\n",
    "    currentDate = startDate\n",
    "    while currentDate < endDate:\n",
    "        yield currentDate\n",
    "        currentDate += delta\n",
    "\n",
    "def preprocess_csv(infile):\n",
    "    \"\"\" reads csv forexite data file, preprocesses it, writes new csv to disk.\n",
    "    \"\"\"\n",
    "    csv_file = dir_from + str(infile)\n",
    "\n",
    "    cpair_list = ['AUDCAD','AUDCHF','AUDJPY','AUDNZD','AUDUSD','CADCHF','CADJPY','CHFJPY','EURAUD',\n",
    "                  'EURCAD','EURCHF','EURGBP','EURJPY','EURNZD','EURRUB','EURSGD','EURUSD','EURZAR',\n",
    "                  'GBPAUD','GBPCAD','GBPCHF','GBPJPY','GBPNZD','GBPUSD','NZDCAD','NZDCHF','NZDJPY',\n",
    "                  'NZDUSD','USDCAD','USDCHF','USDCZK','USDDKK','USDHUF','USDJPY','USDNOK','USDPLN',\n",
    "                  'USDRUB','USDSEK','USDSGD','USDTRY','USDZAR','XAGEUR','XAGUSD','XAUEUR','XAUUSD',\n",
    "                  'USDHKD','USDMXN','EURHKD','EURMXN','EURTRY']\n",
    "    df = pd.read_csv(csv_file, converters={'<TICKER>': lambda x: cpair_list.index(x) + 1,\n",
    "                                           '<DTYYYYMMDD>': lambda x: str(x),\n",
    "                                           '<TIME>': lambda x: str(x)})\n",
    "    df.rename(columns={'<TICKER>': 'ticker_id',\n",
    "                       '<DTYYYYMMDD>': 'date',\n",
    "                       '<TIME>': 'time',\n",
    "                       '<OPEN>': 'rate_open',\n",
    "                       '<HIGH>': 'rate_high',\n",
    "                       '<LOW>': 'rate_low',\n",
    "                       '<CLOSE>': 'rate_close'}, inplace=True)\n",
    "    stamp = df.date + df.time\n",
    "    df['dtime'] = stamp.map(lambda x: datetime.strptime(x, '%Y%m%d%H%M%S'))\n",
    "    del df['date'], df['time']\n",
    "    # WRITING df DIRECTLY TO database IS WAY TOO SLOW, THAT IS WHY WE EXPORT TO CSV FIRST! \n",
    "    # df.to_sql('tbl_forexite', engine, schema=None, if_exists='append', index=False, index_label=None, chunksize=None, dtype=None)\n",
    "    df.to_csv(\"tempfile.csv\", index=False, columns=['ticker_id','dtime', 'rate_open', 'rate_high', 'rate_low', 'rate_close'])\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "010516.txt: preprocessing... OK...loading into database... OK\n",
      "020516.txt: preprocessing... OK...loading into database... OK\n",
      "030516.txt: preprocessing... OK...loading into database... OK\n",
      "040516.txt: preprocessing... OK...loading into database... OK\n",
      "050516.txt: preprocessing... OK...loading into database... OK\n",
      "060516.txt: preprocessing... OK...loading into database... OK\n",
      "070516.txt: does not exist and is skipped.\n",
      "080516.txt: preprocessing... OK...loading into database... OK\n",
      "090516.txt: preprocessing... OK...loading into database... OK\n",
      "100516.txt: preprocessing... OK...loading into database... OK\n",
      "110516.txt: preprocessing... OK...loading into database... OK\n",
      "120516.txt: preprocessing... OK...loading into database... OK\n",
      "130516.txt: preprocessing... OK...loading into database... OK\n",
      "140516.txt: does not exist and is skipped.\n",
      "150516.txt: preprocessing... OK...loading into database... OK\n",
      "160516.txt: preprocessing... OK...loading into database... OK\n",
      "170516.txt: preprocessing... OK...loading into database... OK\n",
      "180516.txt: preprocessing... OK...loading into database... OK\n",
      "190516.txt: preprocessing... OK...loading into database... OK\n",
      "200516.txt: preprocessing... OK...loading into database... OK\n",
      "210516.txt: does not exist and is skipped.\n",
      "220516.txt: preprocessing... OK...loading into database... OK\n",
      "230516.txt: preprocessing... OK...loading into database... OK\n",
      "240516.txt: preprocessing... OK...loading into database... OK\n",
      "250516.txt: preprocessing... OK...loading into database... OK\n",
      "260516.txt: preprocessing... OK...loading into database... OK\n",
      "270516.txt: preprocessing... OK...loading into database... OK\n",
      "280516.txt: does not exist and is skipped.\n",
      "290516.txt: preprocessing... OK...loading into database... OK\n",
      "300516.txt: preprocessing... OK...loading into database... OK\n",
      "310516.txt: preprocessing... OK...loading into database... OK\n",
      "010616.txt: preprocessing... OK...loading into database... OK\n",
      "020616.txt: preprocessing... OK...loading into database... OK\n",
      "030616.txt: preprocessing... OK...loading into database... OK\n",
      "040616.txt: does not exist and is skipped.\n",
      "050616.txt: preprocessing... OK...loading into database... OK\n",
      "060616.txt: preprocessing... OK...loading into database... OK\n",
      "070616.txt: preprocessing... OK...loading into database... OK\n",
      "080616.txt: preprocessing... OK...loading into database... OK\n",
      "090616.txt: preprocessing... OK...loading into database... OK\n",
      "100616.txt: preprocessing... OK...loading into database... OK\n",
      "110616.txt: does not exist and is skipped.\n",
      "120616.txt: preprocessing... OK...loading into database... OK\n",
      "130616.txt: preprocessing... OK...loading into database... OK\n",
      "140616.txt: preprocessing... OK...loading into database... OK\n"
     ]
    }
   ],
   "source": [
    "for date in datespan(start_date, end_date, delta=timedelta(days=1)):\n",
    "    infile = date.strftime('%d%m%y') + '.txt'\n",
    "    if os.path.isfile(dir_from + infile):\n",
    "        print \"%s: preprocessing...\" % infile,\n",
    "        preprocess_csv(infile)\n",
    "        print \"OK...loading into database...\",\n",
    "        _ = engine.execute(\"\"\"COPY tbl_forexite FROM '/home/pieter/projects/quantfxengine/dev/tempfile.csv' WITH DELIMITER ',' CSV HEADER; COMMIT\"\"\")\n",
    "        print \"OK\"\n",
    "    else:\n",
    "        print \"%s: does not exist and is skipped.\" % infile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "dirr = '/home/pieter/projects/quantfxengine/temp/'\n",
    "\n",
    "\n",
    "print os.path.isfile(dirr + '090715.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
